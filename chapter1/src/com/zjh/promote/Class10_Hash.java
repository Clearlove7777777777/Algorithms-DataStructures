package com.zjh.promote;

import java.util.HashMap;
import java.util.Random;

/**
 * Class10_Hash class
 * 哈希函数与哈希表等
 * @author zjh
 * @date 2022/8/24 15:11
 */
public class Class10_Hash {
    /**
     * 哈希函数
     * 1.ta是一个输入域为无穷，输出域为有限这样的函数
     * 2.same in --> same out（相同输入，输出保证一致，证明哈希函数没有随机性）
     * 3.dif int -> same out（哈希碰撞，概率极低极低）
     * 4*：
     *   若把哈希函数的out域想成一个大小固定的面积，假设我们使1000个不同的in得到了1000个out，然后使用一个固定大小的选择域去对out域进行框选
     *      离散性：不论输入的值是相似还是完全不同，其得出的hash值都是完全不同的，即in与out无规律
     *      均匀性：不论在哪个区域进行框选，每个区域中out点的个数几乎都是相同的
     * 实现了上述要求的函数就可以称之为哈希函数，而对于第4点实现的越好我们就说这个哈希函数越优秀（每一种哈希函数的原理都是较为复杂的）
     * 
     * 推论：哈希函数在out域上均匀分布，其对m取模后在0~m-1上也是均匀分布的
     * MD5：2^64-1
     * SHA1：2^128-1
     */

    /**
     * 哈希表
     * 哈希表的原理我们可以简单理解为数组（桶）加链表（改进结构）
     * 对于一个键值对k-v，我们申请一个初始长度为x的数组空间，然后对k做哈希函数，得到的值%x，将k放入数组的此下标中。
     * 哈希值对x取模后是有可能发生碰撞的，若发生碰撞则在已有元素后以链表形式连接新元素，并且规定链表长度上限为y，若某一下标对应的链表长度达到了y（由于哈希函数的均匀性，一个下标对应的链表长度达到了长度上限则其他链表长度也应该几乎满了），则对整个哈希表进行扩容
     * 
     * 具体的语言会有不同的改进方式，但是哈希表的基本原理都是基于上述理论的
     * 
     * 在工程上，我们常说哈希表的增删改查是O(1)级别的
     *  原因是总体上来说对于N的数据，哈希表的时间耗费是N*logxN，于是平均到每一个数据就是logxN，而我们可以把x设定的较大一些，例如10，遍历10个数据依然是很快的，但是logxN可以近似=1了，于是就可以说哈希表是O(1)级别
     *      其次，可以JVM可以通过理线扩容机制，使得用户线程无需等待扩容完成，直接切换到扩容完成的哈希表上工作
     *      
     */

    /**
     * 布隆过滤器
     * 
     * 实际问题：某搜索引擎公司，需要对用户访问的url进行控制，其有一个100亿条url，每条url长度不超过64字节的url黑名单池
     *          用户每访问一个url都需要判断其访问的url是否在黑名单池内，如果在则拒绝访问
     *     **如果采用传统的hashMap方式需要596.04GB的内存，这是十分奢侈的**
     *     
     *     注意：布隆过滤器不支持带有删除操作的业务场景
     *          布隆过滤器会有一定概率的失误率，有可能会将不是黑名单内的数据误报成是黑名单，但一定不会将是黑名单的数据误报成不是黑名单
     *          布隆过滤器可以极大的减少上述场景的内存，但是失误率是无法避免的，只能做到很低
     *          
     *          与单样本的大小无关，即不论是上述的单样本64字节还是1GB，只要能算出哈希值即可，若允许一定程度的失误率，就可以使用布隆过滤器解决
     * 概念：BitMap
     *      int[]是int类型的数组，那么我们是否可以设计一种bit[]，即bit数组，每个数组槽只保存1bit的信息，即0 or 1
     *      我们可以使用int[] arr = new int[10]; 一个int有4个字节，一个字节有8个bit，因此arr可以存储4*8*10 = 320bit信息
     * 布隆过滤器原理：
     *      准备一个长度为m的BitMap，然后准备k个不同的哈希函数
     *      对每个url计算k个哈希函数的值并%m，将得到的k个下标全部设置为1，100亿个数据全部按照上述流程走完后，布隆过滤器就初始化完成了
     *    查询：对于一个url，计算k个哈希值并%m，然后去bitmap中查看如果k个下标全是1，则认为其极大概率是黑名单对象
     *         反之，k个下标中只要有1个下标是0，那么这个对象一定不是黑名单对象
     *      因此，布隆过滤器的失误率与bitmap的长度以及数据量是息息相关的，而k的大小也需要与样本量有关
     *      样本量为N不变的情况下，失误率P与m、k的关系  <img：boloom.png>
     *      **P与m成反比例关系，m越大，失误率越低，但是随着m的逐渐增大，P下降的幅度越来越缓慢，性价比低**
     *      **P与k的关系：k在一开始增大时P会有一个较快速下降过程，但是随着k逐渐增大，bitmap会被急速耗尽，使得整个bitmap趋于全1，因此k的越来越大会导致P急速上升并最终失误率达到100%**
     *      
     * 布隆过滤器的失误率P、样本量N、哈希函数个数k、数组长度m具有函数关系<img: boloom_function.png>
     * 
     */

    /**
     * 一致性哈希原理
     * 用于讨论数据服务器（分布式数据库）怎么组织的问题
     * 
     * 假设我们要对数据进行分库，将一大数据库拆分为3个小数据库，势必牵扯到如何进行拆分的问题，可以使用某个字段进行哈希函数并***%3***的方式（这种方式称为经典模式）
     * 这里就会牵扯到我们该使用哪个字段进行哈希的问题
     *      原则：一定要选取种类比较多，并且高频、中频、低频都有数量这样的字段（比如上述场景我们若使用boolean字段（性别）来进行划分的话，势必导致2台服务器超级负载，一台服务器空转）
     * 设想，当数据量不断增加时，三台服务器势必需要扩容，经典模式的弊端就体现出来，因为只要增加1台服务器，之前所有的取模信息都作废，需要对数据全量重新对4取模，重新放置每一条数据的位置，这个代价是极大的
     * 
     * 一致性哈希就是为了解决上述扩容时经典模式带来的高代价问题  https://zhuanlan.zhihu.com/p/378056811
     * 大致思路：我们将哈希函数的结果想象成一个大环，范围由0-2^64-1，然后我们可以根据物理服务器的ip或者mac地址等计算出一个hash值，在这个大环上打点
     *          然后对于每个数据，计算其hash值，此值在大环上也会有对应的唯一点，顺时针遇到的第一个服务器点就是该数据所归属的服务器
     *          这么做的好处就是在增加或者减少服务器时，代价大大降低
     *              例如原集群有abc三台服务器，在大环上的顺序为a、b、c
     *                  扩容：增加一台服务器d，其hash值在a、b之间，我们只需要将b服务器上的数据根据hash值分割一部分给d即可完成扩容
     *                  减容：将增加的服务器d下线，我们只需要将d的数据全部传输给b即可完成服务器下线
     *     上述思路存在如下问题：
     *          在服务器数量较少的情况下，以上述情况为例，在一开始为服务器计算hash值时，由于只有3台，hash是没有办法做到均匀性的，有可能会造成三个哈希值很接近，也就是三个点在大环上分布不均匀
     *          这就会导致“数据倾斜“，有些节点承载了很重的任务，有些节点却悠闲悠闲
     *          为了解决上述问题，我们引入虚拟节点技术
     *          虚拟节点技术:
     *              对服务器的管辖节点选择不再使用单一的3个ip或者mac地址，而是给每台节点分配1000个虚拟节点，然后去大环上打点，这样大环上就有3000个点，哈希函数在数据量不低的情况下是可以很好的做到均匀性的
     *              每台服务器都保存自己的1000个节点的管辖数据范围索引信息，方便后续查找
     *              对于一条数据，计算其hash值，然后顺时针找最近的虚拟节点，假设为a100，那么就将该数据放入a服务器，查找同理
     *              
     *              扩容：增加一台服务器d，也给其1000个虚拟节点，计算其hash值并在大环上打点，然后去每台服务器保存的索引上找与每个d的虚拟节点最近的结点，例如存在如下a100  d100 c100，那么只需要将原本放置在c机器上的a100-d100的数据挪动到d服务器即可，对每个节点都进行如上操作即可完成扩容，并且很好的解决了数据倾斜的问题
     *              减容：将增加的服务器d下线，同上
     *              
     *          我们还可以控制每台服务器虚拟节点的数量来控制该台服务器的承载量，假设三台服务器性能都一致，那么给三台服务器虚拟节点的数量保持一致，若有一台服务器性能极好或者极差，就增加或者减少虚拟节点的个数即可
     */
    


    /**
     * 例1：给定一个大文件，里面含有40亿个无符号数，每个无符号数的范围为0~2^32-1
     * 问若只给定1G内存，请找出出现次数最多的数字
     * 
     * 思路：
     *      1.若使用传统思路，map<Integer,Integer>计算重复值的次数，那么对于极端情况，40亿个数字都是不同的话，则这个map需要有40亿条记录
     *      而一条map记录需要记录2个Integer共8字节，40亿个数就需要8*40亿字节，（暂且不计算map内部索引占用内存）估算约需要32G内存，超出了最大限制
     *      
     *      **对于上述思路，我们怕的是遇到大量不相同的数，因为不相同的数就会在map里增加一条记录，而对于相同的数字，我们只需要对map的那条记录+1即可**
     *          得出如下思路
     *      2.对文件中的数字进行遍历计算哈希值并%100，将结果=0的放入0号文件，将结果=1的放入1号文件...（利用哈希函数结果的均匀性，防止特征相似的大量不同数字进入同一个小文件）
     *      由于哈希函数的特征，same in -> same out,上述步骤结束后，所有相同的数都被放入了一个文件，然后再对每一个文件进行步骤1即可
     *          设想
     *              最坏情况：大文件中每个数字都不相同，那么由于哈希函数的均匀性，最后得到的100个文件每个文件中数字的数量应该是几乎一致的，那么40亿/100，一个小文件中大约有4000万个数，而4000万*8字节大约为300M内存，完全满足了题意
     *              最好情况：大文件中的每个数字都相同，那么40亿个数字会被放入同一个小文件，然后进行步骤1时map中只会有1条记录：x->40亿次，占用内存为8字节，完全满足题意
     */
    public static void main(String[] args) {
        
        MyHashMap<String,Integer> myHashMap = new MyHashMap<>();
        System.out.println(myHashMap.getRandomKey());
        myHashMap.insert("1",11);
        myHashMap.insert("2",22);
        myHashMap.insert("3",33);

        myHashMap.delete("2");
        
        
        System.out.println(myHashMap.getRandomKey());
        System.out.println(myHashMap.getRandomValue());
        myHashMap.delete("2");
        System.out.println(myHashMap.getRandomKey());
    }

    /**
     * 设计一种结构，在该结构中有如下三个功能：
     * insert(key):将某个key加入到该结构，做到不重复加入
     * delete(key):将原本在结构中的某个key移除
     * getRandom():等概率随机返回结构中的任何一个key.
     * 【要求】
     * Insert、delete和getRandom,方法的时间复杂度都是O(1)
     */
    static class MyHashMap<K, V>{
        private HashMap<K,Integer> keyIndexMap = new HashMap<>();
        private HashMap<Integer, K> indexKeyMap = new HashMap<>();
        
        private HashMap<K, V> map = new HashMap<>();
        
        public void insert(K key, V value){
            keyIndexMap.put(key,map.size());
            indexKeyMap.put(map.size(),key);
            
            map.put(key,value);
        }
        public void delete(K key){
            if (!keyIndexMap.containsKey(key)) {
                return;
            }
            Integer oldIndex = keyIndexMap.get(key);
            indexKeyMap.put(oldIndex, indexKeyMap.get(map.size()-1));   // 保持indexKeyMap上的index是连续的
            keyIndexMap.put(indexKeyMap.get(map.size()-1),oldIndex);
            
            keyIndexMap.remove(key);
            indexKeyMap.remove(map.size()-1);
            map.remove(key);
        }
        
        public K getRandomKey(){
            if (map.size() == 0)
                return null;
            Random random = new Random(System.currentTimeMillis());
            return indexKeyMap.get(random.nextInt(map.size()));
        }

        public V getRandomValue(){
            if (map.size() == 0)
                return null;
            Random random = new Random(System.currentTimeMillis());
            return map.get(indexKeyMap.get(random.nextInt(map.size())));
        }
    }
}
